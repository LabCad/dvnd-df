pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:14, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 7667910-[88 51 46 12 64 18 48 44 62 84 90 65 45 32 97  9 19 78 41 52 39 23 57  6 72
 29  3 61 38  0 80 58 77 86 34 59 14  2 66 13 93 83 70 56 25 30  8 50 82 26
 53 33 31 71 20 76 43 42 91 47 87  1 95 37 81 73 11 24 27 15 49 55 17  5 89
 54 68 40 67 98 69 79 36 35 74 60 63 75  7 99  4 96 94 28 85 16 22 21 10 92] - 7667910
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7f4524568ad0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 7667910-[88 51 46 12 64 18 48 44 62 84 90 65 45 32 97  9 19 78 41 52 39 23 57  6 72
 29  3 61 38  0 80 58 77 86 34 59 14  2 66 13 93 83 70 56 25 30  8 50 82 26
 53 33 31 71 20 76 43 42 91 47 87  1 95 37 81 73 11 24 27 15 49 55 17  5 89
 54 68 40 67 98 69 79 36 35 74 60 63 75  7 99  4 96 94 28 85 16 22 21 10 92]
Final time: 0.900804042816s - Best: 996892-[88 89 16 10 33 80 37 65  7 51 45 75 79 77 30 98 44 32 67 13 64 85 95 15 62
 53 90 76 42 63 58 84 17  5 39 82  2  4 18  1 99 29 50 36 61 52 59 43  3 70
 38 72 14 83 40 23 26 87 78 12  0 49 93 31 96 97 48 69 22 41 21 57 28 35 66
 74  6 73 60 71 56 81 24 19  8 54 86 25  9 20 46 11 27 55 94 91 47 34 68 92]
Value - initial: 7667910, final: 996892, improveup: 7.69181616464

data-line;i;7667910;f;996892;t;0.900804042816;c;-2;fv;[996892L];cv;-1;imp;7.69181616464;type;rvnd;inum;1;w;1

time;0.900804042816;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
