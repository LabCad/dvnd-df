pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:72, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 8271676-[94 85 19 68 59  4 21 30 61 40 37 41 42 87 11 76  0 91 32 23 82 12 89 84 88
 31 66 27 62  5 90 35 34 60 14 36 75 92 17 16 50  2 57  7 10 65 93 18 43 33
 79 28 55 44 46  3 96 67  8 72 54 56 81 24 20  1 15 63 29 78 51 70  6 86 58
 74 26 77 39 49 25 53 71 98 95 13 38 52 80 22 69 97 83 48 73 64  9 45 99 47] - 8271676
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7f960524aad0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 8271676-[94 85 19 68 59  4 21 30 61 40 37 41 42 87 11 76  0 91 32 23 82 12 89 84 88
 31 66 27 62  5 90 35 34 60 14 36 75 92 17 16 50  2 57  7 10 65 93 18 43 33
 79 28 55 44 46  3 96 67  8 72 54 56 81 24 20  1 15 63 29 78 51 70  6 86 58
 74 26 77 39 49 25 53 71 98 95 13 38 52 80 22 69 97 83 48 73 64  9 45 99 47]
Final time: 0.901574134827s - Best: 988115-[94 55 20  9 25 86 54  8 19 24 17 84 58  5 39 82  2  4 52 61 36 50 18  1 99
 29 34 59 43 92  3 70 42 76 53 90 38 72 83 40 23 26 87 78 12  0 49 33 80 37
 65  7 51 45 75 79 77 64 13 67 32 44 98 30 56 71  6 74 66 35 28 57 21 41 88
 69 48 97 96 31 93 16 89 10 22 73 60 68 14 62 15 63 95 85 81 46 11 27 91 47]
Value - initial: 8271676, final: 988115, improveup: 8.37116732364

data-line;i;8271676;f;988115;t;0.901574134827;c;-2;fv;[988115L];cv;-1;imp;8.37116732364;type;rvnd;inum;1;w;1

time;0.901574134827;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
