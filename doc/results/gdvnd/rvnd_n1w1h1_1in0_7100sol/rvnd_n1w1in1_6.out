pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:6, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 8436829-[75 38 47 20 41 48 49 93  9 64 59 79 96 97 76 63 12 91 84 11 51 77 16 81 61
 65 80 86 88  5  0 35 67  1 27 54 94 30 19 18 15 25 21 52 98 37  8 28 43  2
 90 31 55 45 53 10 23 22 73 70 69  6 82 50 74 68 14 17 71 60 13 34 58 85 78
 36 39 83 40 99 33 42 26 66 46 89 72  3 24 32  4 57 44 29 87 62 95 56  7 92] - 8436829
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7f5d8cf1dad0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 8436829-[75 38 47 20 41 48 49 93  9 64 59 79 96 97 76 63 12 91 84 11 51 77 16 81 61
 65 80 86 88  5  0 35 67  1 27 54 94 30 19 18 15 25 21 52 98 37  8 28 43  2
 90 31 55 45 53 10 23 22 73 70 69  6 82 50 74 68 14 17 71 60 13 34 58 85 78
 36 39 83 40 99 33 42 26 66 46 89 72  3 24 32  4 57 44 29 87 62 95 56  7 92]
Final time: 1.78758406639s - Best: 956745-[75 79 77 30 98 44 32 67 13 64 85 95 15 62 53 90 76 42 63 84 58  2 82 39  5
 17 24 19  8 54 25 86 91 47  1 99 29 34 50 18 36 61  4 52 59 43  3 70 38 72
 14 83 40 23 26 87 78 12  0 49 93 31 96 97 48 69 88 89 16 10 33 80 37 65  7
 51 45 60 73 35 66 74  6 71 56 81 46 11 27 20 55 94  9 68 22 41 21 57 28 92]
Value - initial: 8436829, final: 956745, improveup: 8.81826296453

data-line;i;8436829;f;956745;t;1.78758406639;c;-2;fv;[956745L];cv;-1;imp;8.81826296453;type;rvnd;inum;1;w;1

time;1.78758406639;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
