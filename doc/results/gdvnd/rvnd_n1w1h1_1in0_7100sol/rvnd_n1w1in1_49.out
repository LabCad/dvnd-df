pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:49, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 7803974-[69 35 19 24 60 66 89 32 85 48 84 90 93 30 83  6 73 41 82 58 37 14 21 71 20
 70 81 77  2 56  7 44 59 36 22 47 39 75 64 18 51 87 27 34 99 53 52  1 96 55
 28 91 23 33 63  4 29 31 15 65 45 76 67 13 97 46  5 17 12 42 40 98  0 79 80
 25 61 38 10 62 95 78 92 26 43  3 49 88 11 86 54 16 68  8 57  9 50 74 72 94] - 7803974
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7f8905948ad0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 7803974-[69 35 19 24 60 66 89 32 85 48 84 90 93 30 83  6 73 41 82 58 37 14 21 71 20
 70 81 77  2 56  7 44 59 36 22 47 39 75 64 18 51 87 27 34 99 53 52  1 96 55
 28 91 23 33 63  4 29 31 15 65 45 76 67 13 97 46  5 17 12 42 40 98  0 79 80
 25 61 38 10 62 95 78 92 26 43  3 49 88 11 86 54 16 68  8 57  9 50 74 72 94]
Final time: 0.834350824356s - Best: 957880-[69 48 97 96 31 93 16 89 88 10 37 65  7 51 80 33 49  0 12 78 87 26 23 40 83
 72 38 90 53 62 15 76 42 70  3 92 43 59 52 61 36 18 50 29 99  1 47 91 86 25
 54  8  9 94 55 20 27 11 46 19 24 17  5 39 82  2 58 84 95 85 64 13 67 32 44
 98 77 30 56 71 79 75 60 73  6 74 66 35 28 57 21 41 22 45 68 14 63 81  4 34]
Value - initial: 7803974, final: 957880, improveup: 8.14713116466

data-line;i;7803974;f;957880;t;0.834350824356;c;-2;fv;[957880L];cv;-1;imp;8.14713116466;type;rvnd;inum;1;w;1

time;0.834350824356;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
