pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:45, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 9433930-[91 48  1 10 18 61 26 25 81 90 71 50  5 22 74 24 35 67 78 36 97 29 12 66 27
 16 83 64 28 39 87 41 62 43 38 20 31 63 89 92 76  4 13 60 53 69 55  9 73 68
 70 19 58  0 94 57 56 75 98 54 44 30 33 93 95 40 45 46  2 23  7 32 21 65 85
 99 34  6 82 80 79 17 52 14 15 96 42 11 86 84 51  3 49 77 72 37 59 88 47  8] - 9433930
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7efc39409ad0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 9433930-[91 48  1 10 18 61 26 25 81 90 71 50  5 22 74 24 35 67 78 36 97 29 12 66 27
 16 83 64 28 39 87 41 62 43 38 20 31 63 89 92 76  4 13 60 53 69 55  9 73 68
 70 19 58  0 94 57 56 75 98 54 44 30 33 93 95 40 45 46  2 23  7 32 21 65 85
 99 34  6 82 80 79 17 52 14 15 96 42 11 86 84 51  3 49 77 72 37 59 88 47  8]
Final time: 1.08707118034s - Best: 1009695-[91 18  1 99 29 50 36 61 59 52  4  2 82 39  5 58 84 17 24 19  8 54 86 25  9
 81 13 67 32 44 98 30 77 64 85 95 15 62 53 90 76 42  3 43 92 70 38 72 83 40
 23 26 87 78 12  0 49 93 31 96 97 48 69 88 89 16 10 33 80 37 65  7 51 45 22
 41 21 57 28 35 66 74  6 73 60 71 56 79 75 68 14 63 46 11 27 20 55 94 47 34]
Value - initial: 9433930, final: 1009695, improveup: 9.34334625803

data-line;i;9433930;f;1009695;t;1.08707118034;c;-2;fv;[1009695L];cv;-1;imp;9.34334625803;type;rvnd;inum;1;w;1

time;1.08707118034;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
