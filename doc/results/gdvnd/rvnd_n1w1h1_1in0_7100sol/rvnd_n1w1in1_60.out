pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:60, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 8024232-[53 81 90 51 77 22 26  4 19 98 35 82 79 70 52 61 47 54 91 87 16 43 21 44 65
 62 18 50 93 86  2 46 92 14 85  1 89 40 57  5 37 84 33  9 30 17 10 80 27 23
 99 42 74 67 25 56 83 76 96 58 28 75 20 66 48 13  6  0  8 49 38 68 11 71  3
 69 97 34 88  7 12 73 36 55 60 31 29 94 59 24 41 45 72 64 32 78 95 15 39 63] - 8024232
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7ff1217a9ad0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 8024232-[53 81 90 51 77 22 26  4 19 98 35 82 79 70 52 61 47 54 91 87 16 43 21 44 65
 62 18 50 93 86  2 46 92 14 85  1 89 40 57  5 37 84 33  9 30 17 10 80 27 23
 99 42 74 67 25 56 83 76 96 58 28 75 20 66 48 13  6  0  8 49 38 68 11 71  3
 69 97 34 88  7 12 73 36 55 60 31 29 94 59 24 41 45 72 64 32 78 95 15 39 63]
Final time: 0.927227020264s - Best: 999863-[53 90 62 15 76 42 63 58  5 39 82  2  4 52 59 43 92  3 70 38 72 14 83 40 78
 12 87 23 26  0 49 31 93 16 89 97 48 69 88 10 33 80 37 65 51  7 22 41 21 57
 28 35 66 74  6 73 60 75 79 77 30 98 44 32 67 13 64 85 95 84 17 24 19  8 54
 25 86 91 47  1 99 29 34 50 18 36 61  9 94 55 20 46 27 11 81 56 71 68 45 96]
Value - initial: 8024232, final: 999863, improveup: 8.02533147041

data-line;i;8024232;f;999863;t;0.927227020264;c;-2;fv;[999863L];cv;-1;imp;8.02533147041;type;rvnd;inum;1;w;1

time;0.927227020264;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
