pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:79, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 7790595-[47  5 26 27 45 99 72 76 22 14 80 86 18 39 46 19  3 85 17  7 68 44 29 98 13
 60 63 61 34 56 31 53 92 52 24 66 97 64 48 40 11 55 58 79  1 51  6 87 67 37
 59 23 71 36 30 69 12 54  0 93 43 49 33 81  9 89 65 74 28 77 94 78 50 91 35
 57  8 32 96 75  4 82 10 62 20 88 83 21 84 15 70 95 42 41 25 73  2 90 38 16] - 7790595
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7fb993364ad0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 7790595-[47  5 26 27 45 99 72 76 22 14 80 86 18 39 46 19  3 85 17  7 68 44 29 98 13
 60 63 61 34 56 31 53 92 52 24 66 97 64 48 40 11 55 58 79  1 51  6 87 67 37
 59 23 71 36 30 69 12 54  0 93 43 49 33 81  9 89 65 74 28 77 94 78 50 91 35
 57  8 32 96 75  4 82 10 62 20 88 83 21 84 15 70 95 42 41 25 73  2 90 38 16]
Final time: 1.10685992241s - Best: 1007453-[47  1 99 29 50 18 36 61  4 52 59 43 92  3 70 42  2 82 39  5 17 84 58 63 76
 90 53 62 15 95 85 64 13 67 32 44 98 30 77 79 75 45 51  7 65 37 33 80 26 23
 87 78 12  0 49 93 31 96 97 48 69 88 89 10 22 41 21 57 28 35 66 74  6 73 60
 71 56 81 24 19  8 54 86 25  9 20 46 11 27 55 94 91 34 38 72 14 68 83 40 16]
Value - initial: 7790595, final: 1007453, improveup: 7.73296123988

data-line;i;7790595;f;1007453;t;1.10685992241;c;-2;fv;[1007453L];cv;-1;imp;7.73296123988;type;rvnd;inum;1;w;1

time;1.10685992241;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
