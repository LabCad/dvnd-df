pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:68, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 7662192-[71 56 13 44 30 92 55 97 23 74 91 62 40 15 54  9 88 19 61 70 22 35 11 82  0
 27 31 83 75 76 43 33 50 24 58 20  3 95 59  8 41 84 94 47 45 73 64 69 57 77
 63  7 85 68 60 37 10 96 72 46 51 18  1 49 16  5 14 93 38 52 66 99 48  2 67
 81  4 78 87 12 86 28 98 32 42 79 90 34 80 26 29 65 36 21 17 53 25 89 39  6] - 7662192
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7fca091faad0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 7662192-[71 56 13 44 30 92 55 97 23 74 91 62 40 15 54  9 88 19 61 70 22 35 11 82  0
 27 31 83 75 76 43 33 50 24 58 20  3 95 59  8 41 84 94 47 45 73 64 69 57 77
 63  7 85 68 60 37 10 96 72 46 51 18  1 49 16  5 14 93 38 52 66 99 48  2 67
 81  4 78 87 12 86 28 98 32 42 79 90 34 80 26 29 65 36 21 17 53 25 89 39  6]
Final time: 1.77999305725s - Best: 938900-[71 56 30 77 98 44 32 67 13 64 85 95 15 62 53 90 76 42 63 58 84 17  5 39 82
  2 24 19  9  8 54 25 86 91 47  1 99 29 50 18 36 61  4 52 59 43 92  3 70 38
 72 14 83 40 78 12 87 23 26  0 49 33 80 51  7 65 37 10 89 16 93 31 96 97 48
 69 88 22 41 21 57 28 35 66 74  6 73 60 45 68 75 79 81 46 11 27 20 55 94 34]
Value - initial: 7662192, final: 938900, improveup: 8.16081797849

data-line;i;7662192;f;938900;t;1.77999305725;c;-2;fv;[938900L];cv;-1;imp;8.16081797849;type;rvnd;inum;1;w;1

time;1.77999305725;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
