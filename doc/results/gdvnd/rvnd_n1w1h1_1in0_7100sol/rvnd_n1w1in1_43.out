pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:43, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 8267268-[60 29 65 99 82 41  3 67 61 37  1 76 90 94  8 58 20 39 57 87 32 33 22 75 55
 36 71 46 77 85 72 24 74 89 34 30 44 43 18 84 19 56 50  6 49 40 45 95 62 28
 21 63 79 42 13 11 91  2 69 47 15  9 93 86 48 53 23 31 78 73 38 68 70 64 17
 51 26  4 66 12 83 80  0 35  7 52 16 27 88 98 81 14  5 54 59 25 10 96 97 92] - 8267268
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7f1308ed0ad0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 8267268-[60 29 65 99 82 41  3 67 61 37  1 76 90 94  8 58 20 39 57 87 32 33 22 75 55
 36 71 46 77 85 72 24 74 89 34 30 44 43 18 84 19 56 50  6 49 40 45 95 62 28
 21 63 79 42 13 11 91  2 69 47 15  9 93 86 48 53 23 31 78 73 38 68 70 64 17
 51 26  4 66 12 83 80  0 35  7 52 16 27 88 98 81 14  5 54 59 25 10 96 97 92]
Final time: 1.45692801476s - Best: 980630-[60 75 79 71 56 30 77 98 44 32 67 13 64 85 95 84 58  2 82 39  5 17 24 19  8
  9 25 86 54 91 47  1 99 29 50 18 36 61  4 52 59 43 92  3 70 42 76 15 62 53
 90 38 72 83 40 23 26 87 78 12  0 49 33 80 37 65 51  7 22 41 88 10 89 16 93
 31 96 97 48 69 21 57 28 35 66 74  6 73 45 68 14 63 81 46 11 27 20 55 94 34]
Value - initial: 8267268, final: 980630, improveup: 8.43056810418

data-line;i;8267268;f;980630;t;1.45692801476;c;-2;fv;[980630L];cv;-1;imp;8.43056810418;type;rvnd;inum;1;w;1

time;1.45692801476;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
