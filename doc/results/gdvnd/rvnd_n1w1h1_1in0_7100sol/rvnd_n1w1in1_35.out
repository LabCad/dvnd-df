pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:35, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 6881596-[65 83 98  6 24 12 23 44 86 32 30 59  9 52 81 37 26  7 11 60 88 89 33 15 69
 58 68  5 25 77 51 31 97 42  3  8 71 14 39 62 34 94 84 17 35 67 13 70 64 49
 48 57 10 76 46 96 53 61 92 16 93 47 85 21 28 74 40 80 19 75 55 38 18 95  0
 72 63 22 27 79 29 82 91 87 41 43 45 73 90 99  1 78 56 50 36  4 66  2 54 20] - 6881596
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7f591828aad0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 6881596-[65 83 98  6 24 12 23 44 86 32 30 59  9 52 81 37 26  7 11 60 88 89 33 15 69
 58 68  5 25 77 51 31 97 42  3  8 71 14 39 62 34 94 84 17 35 67 13 70 64 49
 48 57 10 76 46 96 53 61 92 16 93 47 85 21 28 74 40 80 19 75 55 38 18 95  0
 72 63 22 27 79 29 82 91 87 41 43 45 73 90 99  1 78 56 50 36  4 66  2 54 20]
Final time: 0.70519900322s - Best: 1116525-[65 51 45 75 79 77 98 30 56 71  6 73 60  7 37 80 33 49  0 26 23 87 12 78 40
 83 14 62 53 90 76 42  3 43 59 52  4  2 82 39  5 58 84 17 24 19  9  8 54 86
 25 94 55 20 27 11 46 81 13 67 32 44 64 85 95 15 63 61 36 50 18 91 47  1 99
 29 34 92 70 38 72 68 10 89 16 93 31 96 97 48 69 88 22 41 21 57 28 35 74 66]
Value - initial: 6881596, final: 1116525, improveup: 6.16340520812

data-line;i;6881596;f;1116525;t;0.70519900322;c;-2;fv;[1116525L];cv;-1;imp;6.16340520812;type;rvnd;inum;1;w;1

time;0.70519900322;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
