pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:22, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 7030100-[59 64 79 49 13 69 12 73 66 40 80 84 81 82 15 71 53 72 34 90 37 32 99 87 68
 85  3 58 95 78 18 41 21  7  6 42 47 93  4 96 65 33 57 76 97 44 67 43 38 74
 23 51 30 11 94 19 83 75 56 70  9 88 77  0 60 22 45 27 17  2 24 89 31 86 54
 48 46 36 61 16 62  1 10 63 35 28 14 25 50 55  5  8 98 20 26 39 92 29 52 91] - 7030100
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7ff07c643ad0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 7030100-[59 64 79 49 13 69 12 73 66 40 80 84 81 82 15 71 53 72 34 90 37 32 99 87 68
 85  3 58 95 78 18 41 21  7  6 42 47 93  4 96 65 33 57 76 97 44 67 43 38 74
 23 51 30 11 94 19 83 75 56 70  9 88 77  0 60 22 45 27 17  2 24 89 31 86 54
 48 46 36 61 16 62  1 10 63 35 28 14 25 50 55  5  8 98 20 26 39 92 29 52 91]
Final time: 0.998923063278s - Best: 993708-[59 52 61 36 18 50 29 99  1 47 91 86 25 54  8 19 24  2 82 39  5 17 84 58 63
 42 76 90 53 62 15 95 85 64 13 67 32 44 98 77 30 56 71 45 51  7 65 37 33 80
 26 23 87 78 12  0 49 93 31 96 97 48 69 88 89 16 10 22 41 21 57 28 35 66 74
  6 73 60 75 79 68 14 83 40 72 38 70  3 92 43  4  9 94 55 20 46 27 11 81 34]
Value - initial: 7030100, final: 993708, improveup: 7.07461346794

data-line;i;7030100;f;993708;t;0.998923063278;c;-2;fv;[993708L];cv;-1;imp;7.07461346794;type;rvnd;inum;1;w;1

time;0.998923063278;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
