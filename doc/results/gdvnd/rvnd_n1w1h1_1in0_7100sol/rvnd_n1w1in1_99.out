pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:99, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 8212780-[ 5 59 30 76 21 64 82 28 16 88 97 40 99 87 68  9 65 73 54 11 80 10 94 62 41
 35 38 29 61 60 75 72 84 51 85 46 66 34 81  3 12 45 25 52 79 92 95 48 91 27
 15 26 69  7 36 96 83  6 71 47 44 37 86 31 98 24 93  2 53 90 56 55 32 50  0
 49 13 22 42 17 63 43 89 23 18 39 19 57 78 33 20  4 70 74 14 77 58  8 67  1] - 8212780
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7fe9deb66ad0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 8212780-[ 5 59 30 76 21 64 82 28 16 88 97 40 99 87 68  9 65 73 54 11 80 10 94 62 41
 35 38 29 61 60 75 72 84 51 85 46 66 34 81  3 12 45 25 52 79 92 95 48 91 27
 15 26 69  7 36 96 83  6 71 47 44 37 86 31 98 24 93  2 53 90 56 55 32 50  0
 49 13 22 42 17 63 43 89 23 18 39 19 57 78 33 20  4 70 74 14 77 58  8 67  1]
Final time: 1.23127698898s - Best: 1010130-[ 5 39 82  2  4 52 61 36 50 18  1 99 29 34 59 43 92  3 70 42 76 53 90 38 72
 40 78 12 87 26 23 83 14 62 15 95 85 64 13 67 32 44 98 77 30 56 71 79 75 45
 51  7 65 37 33 80  0 49 10 89 16 93 31 96 97 48 69 88 22 41 21 57 28 35 66
 74  6 73 60 68 63 58 84 17 24 19  9  8 54 86 25 94 55 20 46 27 11 81 91 47]
Value - initial: 8212780, final: 1010130, improveup: 8.13041885698

data-line;i;8212780;f;1010130;t;1.23127698898;c;-2;fv;[1010130L];cv;-1;imp;8.13041885698;type;rvnd;inum;1;w;1

time;1.23127698898;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
