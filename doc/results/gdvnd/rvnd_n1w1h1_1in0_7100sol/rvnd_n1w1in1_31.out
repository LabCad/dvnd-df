pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:31, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 8419723-[30 26  7 15 56 49 11 95 65 59 61 96 72 45 19 71  6 50 86 38 90 21 52 12 14
 99 10 93 70 89 23 37 76 79 34 18 91 17 73 53  9 35  2 48 75 47 40 58 42 43
 44 97 57 25 87 54 60 32 66 46 63 85 33 28 24 31 13 55 41 81 36  0 62  1 64
 94 29 16 39 98 74  5 82 83 27 22  3  8 68 51 88 84 67 92 69 78 80 77  4 20] - 8419723
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7fde1cb4cad0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 8419723-[30 26  7 15 56 49 11 95 65 59 61 96 72 45 19 71  6 50 86 38 90 21 52 12 14
 99 10 93 70 89 23 37 76 79 34 18 91 17 73 53  9 35  2 48 75 47 40 58 42 43
 44 97 57 25 87 54 60 32 66 46 63 85 33 28 24 31 13 55 41 81 36  0 62  1 64
 94 29 16 39 98 74  5 82 83 27 22  3  8 68 51 88 84 67 92 69 78 80 77  4 20]
Final time: 2.16759586334s - Best: 1036516-[30 77 64 13 67 32 44 98 56 71  6 73 60 45 51  7 65 37 80 33 10 16 89 88 69
 48 97 96 31 93 49  0 12 78 87 26 23 40 83 14 72 38 90 53 62 15 95 85 84 58
  5 39 82  2 63 76 42 70  3 92 43 59 61 36 50 18 91 47  1 99 29 34 52  4 54
 86 25  8  9 94 55 20 27 11 46 19 24 81 17 68 79 75 74 66 35 28 57 21 41 22]
Value - initial: 8419723, final: 1036516, improveup: 8.12309988461

data-line;i;8419723;f;1036516;t;2.16759586334;c;-2;fv;[1036516L];cv;-1;imp;8.12309988461;type;rvnd;inum;1;w;1

time;2.16759586334;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
