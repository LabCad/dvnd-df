pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:25, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 7838209-[61 92 23 64 54 95 83 25 71 98 11 37 21 59 26 17 52 86 99 41 50  5 48 65  1
 28  9 58 66 67  6 57 79  7 40 80 74 81 63 89 14  3 96 94 46 85 22 51 70 32
 88 84 56  2 43  8 24 76 55 27 36 38 75 68 33 20 13 60 62 69 39 15 10  0 87
 73 93 78 30 82 42 19 90 35 53 29 91 97 45 34 47 12 16 77 44 31 18  4 49 72] - 7838209
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7ffb11689ad0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 7838209-[61 92 23 64 54 95 83 25 71 98 11 37 21 59 26 17 52 86 99 41 50  5 48 65  1
 28  9 58 66 67  6 57 79  7 40 80 74 81 63 89 14  3 96 94 46 85 22 51 70 32
 88 84 56  2 43  8 24 76 55 27 36 38 75 68 33 20 13 60 62 69 39 15 10  0 87
 73 93 78 30 82 42 19 90 35 53 29 91 97 45 34 47 12 16 77 44 31 18  4 49 72]
Final time: 0.907948970795s - Best: 963824-[61 34 29 99 50 36 18  1 47 91 54 86 25  9  8 19 24 17 84 58  5 39 82  2  4
 52 59 43 92  3 70 42 76 90 53 62 15 95 85 64 13 67 32 44 98 77 30 56 71 75
 79 68 14 83 40 78 12 87 23 26  0 49 33 80 37 65 51  7 22 10 89 16 93 31 96
 97 48 69 88 41 21 57 28 35 66 74  6 73 60 45 72 38 63 81 46 20 55 94 27 11]
Value - initial: 7838209, final: 963824, improveup: 8.13240695397

data-line;i;7838209;f;963824;t;0.907948970795;c;-2;fv;[963824L];cv;-1;imp;8.13240695397;type;rvnd;inum;1;w;1

time;0.907948970795;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
