pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:88, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 8654316-[ 7 59 90 13 14 28  2 34 32 95 58 68 54 51 85 91 11 88 33  1 80 25  8 23 70
 36 42  6 82 64 97 74 87 56 19 65 45 40 20 86 84 49 81 27 16  5 75 57 18 31
 77  4 93 62 55 66 29 73 53 50 78 79  9 15 22 47 63 94 89 44 99  3 83 43 72
 30 39 76 48 92 17 37 21 69 35 52 60 10 12 38 46 71 24 61 67 41  0 96 26 98] - 8654316
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7f6242f4fad0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 8654316-[ 7 59 90 13 14 28  2 34 32 95 58 68 54 51 85 91 11 88 33  1 80 25  8 23 70
 36 42  6 82 64 97 74 87 56 19 65 45 40 20 86 84 49 81 27 16  5 75 57 18 31
 77  4 93 62 55 66 29 73 53 50 78 79  9 15 22 47 63 94 89 44 99  3 83 43 72
 30 39 76 48 92 17 37 21 69 35 52 60 10 12 38 46 71 24 61 67 41  0 96 26 98]
Final time: 0.832615852356s - Best: 955256-[ 7 51 65 37 80 33 49  0 12 78 87 26 23 40 83 72 38 90 53 62 15 76 42 70  3
 92 43 59 52  4 61 36 18 50 34 29 99  1 47 91 86 25 54  8 19 24 17  5 39 82
  2 58 84 95 85 64 13 67 32 44 77 30 56 71 79 75 60 73  6 74 66 35 28 57 21
 69 48 97 96 31 93 16 89 10 88 41 22 45 68 14 63  9 94 55 20 46 27 11 81 98]
Value - initial: 8654316, final: 955256, improveup: 9.05968243068

data-line;i;8654316;f;955256;t;0.832615852356;c;-2;fv;[955256L];cv;-1;imp;9.05968243068;type;rvnd;inum;1;w;1

time;0.832615852356;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
