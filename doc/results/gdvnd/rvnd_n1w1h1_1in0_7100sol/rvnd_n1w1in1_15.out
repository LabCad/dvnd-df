pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:15, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 8673786-[35 17  3 99  2 42 54 44 51 66 19 40  6 29 32 49 94 65 88 95 56 93 59 90 18
 46 10 27 36 23  4 28 80 57  1  5 97 82 96 25 76 74 41 83 31 68  8 50 79 64
 62 67 13 78 73  0 14 77 91 84 60 52 55 24 21 20 85 58 92 61 30 86 87 12 48
 38 63 37 53 81 72 39 98 71 70 89  9 33 22 15 69  7 16 75 34 11 26 43 47 45] - 8673786
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7f6233885ad0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 8673786-[35 17  3 99  2 42 54 44 51 66 19 40  6 29 32 49 94 65 88 95 56 93 59 90 18
 46 10 27 36 23  4 28 80 57  1  5 97 82 96 25 76 74 41 83 31 68  8 50 79 64
 62 67 13 78 73  0 14 77 91 84 60 52 55 24 21 20 85 58 92 61 30 86 87 12 48
 38 63 37 53 81 72 39 98 71 70 89  9 33 22 15 69  7 16 75 34 11 26 43 47 45]
Final time: 1.97159814835s - Best: 978606-[35 66 74  6 73 60 75 79 77 30 98 44 32 67 13 64 85 95 84 58 39 82  5 17 24
 19  8 54 25 86 91 47  1 99 29 34 50 18 36 61 52 59 43 92  3 70 42 76 53 90
 38 72 83 40 23 26 87 78 12  0 49 93 31 96 97 48 69 88 89 16 10 33 80 37 65
  7 51 45 68 14 62 15 63  2  4  9 94 55 20 46 27 11 81 56 71 22 41 57 28 21]
Value - initial: 8673786, final: 978606, improveup: 8.86340978903

data-line;i;8673786;f;978606;t;1.97159814835;c;-2;fv;[978606L];cv;-1;imp;8.86340978903;type;rvnd;inum;1;w;1

time;1.97159814835;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
