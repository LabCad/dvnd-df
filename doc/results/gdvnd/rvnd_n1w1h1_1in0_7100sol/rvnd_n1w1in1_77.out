pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:77, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 7476343-[14 91 82 62 89 16 95  4 17 96 12 83 86 49  7 43 11 45 74 34 40 99  2  3 66
 44 57  6 25 54 58 71 32 81 24 35 33 84 77 53 30  1 79 98 31 72 87 28 19 59
 75 70 52 94 93 47 85  8 60 23 10  5 61 50 67 80 92 15 20 73 37 48 76 90 68
 26  0 13 42 63 55 64 88 21 38 51 29 39 41 69 27  9 22 18 36 56 65 46 97 78] - 7476343
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7f534b148ad0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 7476343-[14 91 82 62 89 16 95  4 17 96 12 83 86 49  7 43 11 45 74 34 40 99  2  3 66
 44 57  6 25 54 58 71 32 81 24 35 33 84 77 53 30  1 79 98 31 72 87 28 19 59
 75 70 52 94 93 47 85  8 60 23 10  5 61 50 67 80 92 15 20 73 37 48 76 90 68
 26  0 13 42 63 55 64 88 21 38 51 29 39 41 69 27  9 22 18 36 56 65 46 97 78]
Final time: 1.83022689819s - Best: 1013183-[14 62 53 90 76 42 63 84 58  5 39 82  2  4 52 61 36 50 18  1 99 29 34 59 43
 92  3 70 38 72 83 40 23 26 87 78 12  0 49 93 31 96 97 48 69 88 89 16 10 33
 80 37 65 51  7 22 41 21 57 28 35 66 74  6 73 60 75 79 77 30 98 44 32 67 13
 81 24 19  9  8 54 86 25 94 55 20 27 11 46 17 95 85 64 56 71 45 68 15 91 47]
Value - initial: 7476343, final: 1013183, improveup: 7.37906478889

data-line;i;7476343;f;1013183;t;1.83022689819;c;-2;fv;[1013183L];cv;-1;imp;7.37906478889;type;rvnd;inum;1;w;1

time;1.83022689819;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
