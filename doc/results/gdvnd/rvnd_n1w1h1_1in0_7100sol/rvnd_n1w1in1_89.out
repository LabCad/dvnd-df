pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:89, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 8460829-[21 73 40 93 84 25  9 22 92 14 54 39 98 75 11 33 20 51  8 90 52 29 49 82 30
 59 60 57 61 48 88 10 91 69 66 55 87 64 78 26  1  5 13 35 27 19 62 44 95 28
  3 71 70 58 89 18 67 32 96 42 24 12 83 46 45 79 65 47 37 63 38 86 53 50 36
 16  2 77 15 81 94 76 74 31 43 68  0 23 80  7 34 99 97  4 56 72 17  6 85 41] - 8460829
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7fdd1519ead0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 8460829-[21 73 40 93 84 25  9 22 92 14 54 39 98 75 11 33 20 51  8 90 52 29 49 82 30
 59 60 57 61 48 88 10 91 69 66 55 87 64 78 26  1  5 13 35 27 19 62 44 95 28
  3 71 70 58 89 18 67 32 96 42 24 12 83 46 45 79 65 47 37 63 38 86 53 50 36
 16  2 77 15 81 94 76 74 31 43 68  0 23 80  7 34 99 97  4 56 72 17  6 85 41]
Final time: 1.24192404747s - Best: 1021855-[21 41 22  7 51 65 37 80 33 10 88 89 16 93 49  0 26 23 87 12 78 40 83 14 72
 38 70 92 43  3 42 76 90 53 62 15 63 58  5 39 82  2  4 52 59 61 36 18 50 34
 29 99  1 47 91 86 25 54  8  9 94 55 20 46 19 24 17 84 95 85 64 13 67 32 44
 98 77 30 56 71 79 75 60 73  6 74 66 35 28 57 69 48 97 96 31 45 68 81 11 27]
Value - initial: 8460829, final: 1021855, improveup: 8.27987238894

data-line;i;8460829;f;1021855;t;1.24192404747;c;-2;fv;[1021855L];cv;-1;imp;8.27987238894;type;rvnd;inum;1;w;1

time;1.24192404747;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
