pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:38, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 8713006-[ 3  7 73 13 21 26 20 64 67 37 81 28 63 96 57 91 10 46 25 93 66 60  2  4 98
 36  0 75 76 50 77 82 24 15  6 43 52 61 30 94 70 45 87 56  8 12 84 97 53 71
  5 41 14 19 80 34 35 72 83 23 59 69  1 42 85 79 31 11 40 54 47 39 29 68 16
 44 88 51 65 78 92 38 58 32 49  9 33 95 99 55 22 86 62 17 27 48 90 74 18 89] - 8713006
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7fd5afc68ad0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 8713006-[ 3  7 73 13 21 26 20 64 67 37 81 28 63 96 57 91 10 46 25 93 66 60  2  4 98
 36  0 75 76 50 77 82 24 15  6 43 52 61 30 94 70 45 87 56  8 12 84 97 53 71
  5 41 14 19 80 34 35 72 83 23 59 69  1 42 85 79 31 11 40 54 47 39 29 68 16
 44 88 51 65 78 92 38 58 32 49  9 33 95 99 55 22 86 62 17 27 48 90 74 18 89]
Final time: 1.94979310036s - Best: 1065172-[ 3 92 43 59 52 61 36 18 50 34 29 99  1 47 91 25 86 54  8  9 94 55 20 27 11
 46 19 24 17  5 82 39 58 84 63 42 76 15 62 53 90 38 72 83 40 23 26 87 78 12
 49  0 31 96 93 16 89 48 97 69 88 10 80 33 37 65  7 51 45 75 79 77 30 98 44
 67 32 13 64 85 95 71 56  6 74 35 66 28 57 21 22 41 73 60 68 14 70  4 81  2]
Value - initial: 8713006, final: 1065172, improveup: 8.17990521719

data-line;i;8713006;f;1065172;t;1.94979310036;c;-2;fv;[1065172L];cv;-1;imp;8.17990521719;type;rvnd;inum;1;w;1

time;1.94979310036;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
