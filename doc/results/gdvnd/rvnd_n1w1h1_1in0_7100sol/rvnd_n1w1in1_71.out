pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:71, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 8196093-[75 59 77 20 55  0 28 60 49 56 81 10 64 91  2 67 31 66 57 24  7 23 68 46 35
 21 54 73 63 72 16 27 85 11 29 17 93 26 69 83 41 13 65 42 37 88 34 71 79 39
 50 78 36  1 22 19  9 45 52 89 96 92  4 94 58 86 12 33 99 53 61 14 38  6  5
 44 97 15 82 43 48 70  3  8 25 47 90 76 98 84 32 95 62 80 30 74 87 51 40 18] - 8196093
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7f0cc2e3ead0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 8196093-[75 59 77 20 55  0 28 60 49 56 81 10 64 91  2 67 31 66 57 24  7 23 68 46 35
 21 54 73 63 72 16 27 85 11 29 17 93 26 69 83 41 13 65 42 37 88 34 71 79 39
 50 78 36  1 22 19  9 45 52 89 96 92  4 94 58 86 12 33 99 53 61 14 38  6  5
 44 97 15 82 43 48 70  3  8 25 47 90 76 98 84 32 95 62 80 30 74 87 51 40 18]
Final time: 0.906038999557s - Best: 990169-[75 79 77 30 98 44 32 67 13 64 85 95 84 58  5 39 82  2 63 76 90 53 62 14 83
 40 78 12 87 23 26  0 49 93 31 96 97 48 69 88 89 16 10 33 80 37 65  7 51 45
 68 15 42 70  3 92 43 59 52  4 61 36 18 50 34 29 99  1 47 91 54 86 25 94 55
 27 11 46 20  9  8 19 24 17 81 56 71 60 73  6 74 66 35 28 57 21 41 22 72 38]
Value - initial: 8196093, final: 990169, improveup: 8.27746879573

data-line;i;8196093;f;990169;t;0.906038999557;c;-2;fv;[990169L];cv;-1;imp;8.27746879573;type;rvnd;inum;1;w;1

time;0.906038999557;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
