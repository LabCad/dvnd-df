pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:90, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 8372941-[99 62 91 50 80 58 83 95  9 22 86  5 33 78 87 15 73 30 10 14 25  0  2  3 13
 31  1 55 53 35 28 45 74 97  8 98 76 11 52 36 85 54 93 47 26  6 12 94 65 71
 81 23 89 24 37 70 48 82 21 96 40 90 67  4 77 63 57 46 39 27 29 17 51 19 72
 79 61 42 34 68 32 44 75 69 64 84 88 38 92 49 20 66 16 56  7 59 43 60 41 18] - 8372941
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7f6d7be7bad0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 8372941-[99 62 91 50 80 58 83 95  9 22 86  5 33 78 87 15 73 30 10 14 25  0  2  3 13
 31  1 55 53 35 28 45 74 97  8 98 76 11 52 36 85 54 93 47 26  6 12 94 65 71
 81 23 89 24 37 70 48 82 21 96 40 90 67  4 77 63 57 46 39 27 29 17 51 19 72
 79 61 42 34 68 32 44 75 69 64 84 88 38 92 49 20 66 16 56  7 59 43 60 41 18]
Final time: 1.14075303078s - Best: 973624-[99 29 50 36 18  1 47 91 54 86 25  9  8 19 24 17 84 58  5 39 82  2  4 52 59
 43 92  3 70 42 76 53 90 38 72 83 40 23 26 87 78 12  0 49 93 31 96 97 48 69
 88 89 16 10 33 80 37 65  7 51 45 68 14 62 15 95 85 64 13 67 32 44 98 77 30
 56 71 79 75 60 73  6 74 66 35 28 57 21 41 22 63 81 46 11 27 20 55 94 61 34]
Value - initial: 8372941, final: 973624, improveup: 8.59976849379

data-line;i;8372941;f;973624;t;1.14075303078;c;-2;fv;[973624L];cv;-1;imp;8.59976849379;type;rvnd;inum;1;w;1

time;1.14075303078;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
