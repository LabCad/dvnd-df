pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:13, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 8427985-[50 68 16 63 27 45  9 77 61 48 57 82 71 95 14 15 10 33 83 91 13  1 22  0 53
 89 76 84 26 55 56  7 78  2 25 64 18 11 80 46  6  3 96 90 86 36 34 98 97 39
 52 37 79 93 62 38  8 35 23 65 88 54 72 42  4 30 69 85  5 66 32 60 12 44 51
 24 19 94 58 28 43 81 73 59 29 47 31 99 40 41 67 75 17 21 87 74 70 92 49 20] - 8427985
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7f536c1dbad0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 8427985-[50 68 16 63 27 45  9 77 61 48 57 82 71 95 14 15 10 33 83 91 13  1 22  0 53
 89 76 84 26 55 56  7 78  2 25 64 18 11 80 46  6  3 96 90 86 36 34 98 97 39
 52 37 79 93 62 38  8 35 23 65 88 54 72 42  4 30 69 85  5 66 32 60 12 44 51
 24 19 94 58 28 43 81 73 59 29 47 31 99 40 41 67 75 17 21 87 74 70 92 49 20]
Final time: 1.79210805893s - Best: 982717-[50 36 18 99  1 47 91 86 25 54  8 19 24  2 82 39  5 17 84 58 63 42 76 90 53
 62 15 95 85 64 13 67 32 44 98 77 30 56 71 79 75 60 73  6 74 66 35 28 57 21
 69 48 97 96 31 93 16 89 88 10 37 65  7 51 80 33 49  0 12 78 87 26 23 40 83
 14 72 38 70  3 92 43 59 52  4 61 34 29  9 94 55 20 46 27 11 81 68 45 22 41]
Value - initial: 8427985, final: 982717, improveup: 8.57620759588

data-line;i;8427985;f;982717;t;1.79210805893;c;-2;fv;[982717L];cv;-1;imp;8.57620759588;type;rvnd;inum;1;w;1

time;1.79210805893;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
