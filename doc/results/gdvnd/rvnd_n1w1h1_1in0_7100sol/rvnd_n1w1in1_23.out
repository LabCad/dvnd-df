pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:23, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 8127995-[89 93 27 39 66 34 14 57 41 50 65 64 23 74 32 28 33 61 25  2 22  1 55 83 24
 52 20 53 17 98 43 76 75  0 69 96 37  9  7 13 30  5 85 58  6 18  3 78 12 72
 47 31 86 54 38 87 73  8 62 26 71 29 51 88 15 49 97 91 59 35 90 67 99 80 70
 46 60 82 44 81  4 36 94 11 68 40 79 77 10 42 19 56 21 45 16 48 84 95 63 92] - 8127995
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7fba3a962ad0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 8127995-[89 93 27 39 66 34 14 57 41 50 65 64 23 74 32 28 33 61 25  2 22  1 55 83 24
 52 20 53 17 98 43 76 75  0 69 96 37  9  7 13 30  5 85 58  6 18  3 78 12 72
 47 31 86 54 38 87 73  8 62 26 71 29 51 88 15 49 97 91 59 35 90 67 99 80 70
 46 60 82 44 81  4 36 94 11 68 40 79 77 10 42 19 56 21 45 16 48 84 95 63 92]
Final time: 1.14683413506s - Best: 1045305-[89 10 88 69 48 97 16 93 31  0 49 33 80 37 65  7 51 26 23 87 12 78 40 83 14
 62 53 90 38 70  3 43 42 76 63 58 84 17  5 39 82  2  4 52 59 61 34 29 99 50
 36 18  1 47 91 86 25 54  8  9 94 55 20 27 11 46 19 24 81 13 67 32 44 98 30
 77 79 75 56 71  6 74 66 35 28 57 21 41 22 73 60 45 68 64 85 95 15 72 96 92]
Value - initial: 8127995, final: 1045305, improveup: 7.77571617853

data-line;i;8127995;f;1045305;t;1.14683413506;c;-2;fv;[1045305L];cv;-1;imp;7.77571617853;type;rvnd;inum;1;w;1

time;1.14683413506;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
