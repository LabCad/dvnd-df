pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:39, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 8228613-[40 95 21 54 60 97 31 76 88 47  7 68 41 61 22 42  0 36 48 35 30 46 83 75 65
 50 24 72 74 73  4 25 90 26 81 69 55 18 56 53 87 33 37 32 29 23 12 10 64 78
 45 99 20 63 43 84 62 14 44 77 16 49  3 80 71 15 92 39 52 59 17 51 96 89  6
 82 86 34  9 91 19  1 98 57 85  2 79 28 93 11  5 67 13 58 94 27 66  8 38 70] - 8228613
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7f0696cf0ad0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 8228613-[40 95 21 54 60 97 31 76 88 47  7 68 41 61 22 42  0 36 48 35 30 46 83 75 65
 50 24 72 74 73  4 25 90 26 81 69 55 18 56 53 87 33 37 32 29 23 12 10 64 78
 45 99 20 63 43 84 62 14 44 77 16 49  3 80 71 15 92 39 52 59 17 51 96 89  6
 82 86 34  9 91 19  1 98 57 85  2 79 28 93 11  5 67 13 58 94 27 66  8 38 70]
Final time: 0.760461807251s - Best: 997033-[40 78 12 87 26 23 83 72 38 90 53 62 15 76 42 70  3 92 43 59 52  4 61 36 50
 29 99  1 18 91 54 86 25  9  8 19 24  2 82 39  5 17 84 58 63 95 85 64 13 67
 32 44 98 30 77 79 75 45 51  7 65 37 33 80  0 49 10 89 16 93 31 96 97 48 69
 88 22 41 21 57 28 35 66 74  6 73 60 71 56 81 46 11 27 20 55 94 47 34 68 14]
Value - initial: 8228613, final: 997033, improveup: 8.25309994754

data-line;i;8228613;f;997033;t;0.760461807251;c;-2;fv;[997033L];cv;-1;imp;8.25309994754;type;rvnd;inum;1;w;1

time;0.760461807251;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
