pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:94, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 7993550-[12 88 92 97 75 64 66  2 65 72 13 95 47 84 73  8 40 21 52 56 33 34 70 98 32
 53 69 96 22 60 42 31 87  7 76 89 82 38 37 62 25 54 17 29 51 74 11 57 16 81
  3 24  5  0 36 27 58 67 10 49 46 85 35 30 45 80 77 14 48  9 83 94 15 28 50
 26 99 39 55 44 61 68 19 18 43 86 90 59 63  4 93 71 41  6 78 79 23 91 20  1] - 7993550
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7f5a0b38cad0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 7993550-[12 88 92 97 75 64 66  2 65 72 13 95 47 84 73  8 40 21 52 56 33 34 70 98 32
 53 69 96 22 60 42 31 87  7 76 89 82 38 37 62 25 54 17 29 51 74 11 57 16 81
  3 24  5  0 36 27 58 67 10 49 46 85 35 30 45 80 77 14 48  9 83 94 15 28 50
 26 99 39 55 44 61 68 19 18 43 86 90 59 63  4 93 71 41  6 78 79 23 91 20  1]
Final time: 0.789506912231s - Best: 987863-[12 78 87 23 26 33 80 37 65  7 51 45 75 71 56 30 77 98 44 32 67 13 64 95 85
 15 62 90 53 76 42 63 84 58  2 82  5 39 17 24 19  9  8 54 25 86 91 47  1 29
 99 50 18 36 61  4 52 59 43 92  3 70 38 72 14 83 40  0 49 10 16 89 93 31 96
 97 48 69 88 22 41 21 28 35 66 74  6 73 60 68 81 46 11 27 20 55 94 34 79 57]
Value - initial: 7993550, final: 987863, improveup: 8.09175968733

data-line;i;7993550;f;987863;t;0.789506912231;c;-2;fv;[987863L];cv;-1;imp;8.09175968733;type;rvnd;inum;1;w;1

time;0.789506912231;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
