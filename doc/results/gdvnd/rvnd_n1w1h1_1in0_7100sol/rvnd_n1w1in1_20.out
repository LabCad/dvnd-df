pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:20, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 8240949-[ 2 18 29 48 27 24 65 94 87 86  3 47 19  1 36 78 72 31  4 99 90 49 28  6 89
 23 71 50 96 21 52 14 17 64  7 84 13 45 82 98 69 59 56 51 15 57 73  5 20 81
 25 75 30  0 63 35 62 12 10 58 40 37 44 43 88 91 42 97 16  9 32 70 38 92 66
 61 74 93 54 41 68  8 77 85 11 34 46 26 33 53 83 95 22 76 79 60 80 39 67 55] - 8240949
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7fa04e59aad0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 8240949-[ 2 18 29 48 27 24 65 94 87 86  3 47 19  1 36 78 72 31  4 99 90 49 28  6 89
 23 71 50 96 21 52 14 17 64  7 84 13 45 82 98 69 59 56 51 15 57 73  5 20 81
 25 75 30  0 63 35 62 12 10 58 40 37 44 43 88 91 42 97 16  9 32 70 38 92 66
 61 74 93 54 41 68  8 77 85 11 34 46 26 33 53 83 95 22 76 79 60 80 39 67 55]
Final time: 1.09555411339s - Best: 974963-[ 2 82 39  5 58 84 17 24 19  8 54 25 86 91 47  1 99 29 34 50 18 36 61 52 59
 43 92  3 70 42 76 15 62 53 90 38 72 83 40 23 26 87 78 12  0 49 33 80 37 10
 89 16 93 31 96 97 48 69 88 41 22 65  7 51 45 60 73 35 66 74  6 71 56 30 77
 98 44 32 67 13 64 85 95 81 46 11 27 20 55 94  9  4 63 14 68 79 75 21 57 28]
Value - initial: 8240949, final: 974963, improveup: 8.45257614904

data-line;i;8240949;f;974963;t;1.09555411339;c;-2;fv;[974963L];cv;-1;imp;8.45257614904;type;rvnd;inum;1;w;1

time;1.09555411339;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
