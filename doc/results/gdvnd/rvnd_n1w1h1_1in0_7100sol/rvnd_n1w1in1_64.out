pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:64, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 8107245-[92 13 26 12 94 77 24 87 62 82 29 93 84 16 15  7 41 23 18 90 35 51 22 76 98
 56 58 88 72 36 74 52 30 32 20 28 27 25 80 71 75 44 64  2 67 65 68 38 86 19
 59 57  5 73 53 81 55 85  1  0 17 37 31 14 33 21 11 99 39 42 69  4 63 47 96
 70 10  8  9 97 61 78 83 49 43 50 60 46 89 40 66 95 34 45 54  3 91 79  6 48] - 8107245
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7f1722dfead0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 8107245-[92 13 26 12 94 77 24 87 62 82 29 93 84 16 15  7 41 23 18 90 35 51 22 76 98
 56 58 88 72 36 74 52 30 32 20 28 27 25 80 71 75 44 64  2 67 65 68 38 86 19
 59 57  5 73 53 81 55 85  1  0 17 37 31 14 33 21 11 99 39 42 69  4 63 47 96
 70 10  8  9 97 61 78 83 49 43 50 60 46 89 40 66 95 34 45 54  3 91 79  6 48]
Final time: 1.02829194069s - Best: 977609-[92 43  3 70 42 76 90 53 62 15 95 85 64 13 67 32 44 98 77 30 56 71 79 75 45
 51  7 65 37 80 33 10 16 89 88 69 48 97 96 31 93 49  0 26 23 87 12 78 40 83
 14 72 38 63 58  5 39 82  2  4 52 59 61 36 18 50 34 29 99  1 47 91 86 25 54
  8 19 24  9 94 55 20 46 27 11 81 17 84 68 60 73  6 74 66 35 28 57 21 41 22]
Value - initial: 8107245, final: 977609, improveup: 8.29293204134

data-line;i;8107245;f;977609;t;1.02829194069;c;-2;fv;[977609L];cv;-1;imp;8.29293204134;type;rvnd;inum;1;w;1

time;1.02829194069;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
