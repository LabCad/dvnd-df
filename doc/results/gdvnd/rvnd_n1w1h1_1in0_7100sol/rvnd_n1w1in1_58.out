pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:58, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 7436230-[86  1 93 60 13 53 49  7 23 39 54 11 83  8 81 38 62 91 27 25 21 80 65 41 47
  4 59 32 26 18  9 69 88 57  5 90 34 73 22 10 67  3 46 85 70 71  2 77 51 89
 50 15 96 64  6 82  0 55 42 61 99 28 58 24 56 19 68 92 48 95 75 17 43 87 31
 12 20 37 74 29 63 97 40 72 45 98 84 30 44 94 78 16 35 79 66 36 14 33 76 52] - 7436230
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7fbb38b62ad0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 7436230-[86  1 93 60 13 53 49  7 23 39 54 11 83  8 81 38 62 91 27 25 21 80 65 41 47
  4 59 32 26 18  9 69 88 57  5 90 34 73 22 10 67  3 46 85 70 71  2 77 51 89
 50 15 96 64  6 82  0 55 42 61 99 28 58 24 56 19 68 92 48 95 75 17 43 87 31
 12 20 37 74 29 63 97 40 72 45 98 84 30 44 94 78 16 35 79 66 36 14 33 76 52]
Final time: 1.65869688988s - Best: 1029640-[86 25 91 47  1 99 29 50 18 36 61 54  8 19 24  5 39 82  2  4 52 59 43 92  3
 70 42 76 63 58 84 17 81 13 67 32 44 98 30 77 64 85 95 15 62 53 90 38 72 14
 83 40 23 26 87 78 12  0 49 93 31 96 97 48 69 88 89 16 10 33 80 37 65  7 51
 45 22 41 21 57 28 35 66 74  6 73 60 75 79 71 56 11 27 46 20 55 94  9 34 68]
Value - initial: 7436230, final: 1029640, improveup: 7.22216502855

data-line;i;7436230;f;1029640;t;1.65869688988;c;-2;fv;[1029640L];cv;-1;imp;7.22216502855;type;rvnd;inum;1;w;1

time;1.65869688988;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
