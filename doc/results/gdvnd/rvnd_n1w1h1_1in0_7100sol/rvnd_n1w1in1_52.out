pydf_home: /home/rodolfo/git/Sucuri
simple_pycuda_home: /home/rodolfo/git/simple-pycuda
wamca2016path: /home/rodolfo/git/wamca2016/
localpath: /home/rodolfo/git/dvnd-df/code/dvnd-df/src/
WAMCAPATH:/home/rodolfo/git/wamca2016/
param: {solution_index:1, solution_instance_index:52, multi_gpu:True, goal:False, problem_name:ml, number_of_moves:10, device_count:2, solver:rvnd, mpi_enabled:True, workers:1}
Using already created file:  wamca2016lib.so
Size: 100 - file name: 02_kroD100.tsp

Value - initial: 8056728-[45 55 87 13 19 23 58 53 74 14 75 90 17 66 70 72 42 56 46 50 89 21  1 96 60
 52 79 20 27 85 49 88 61 26 95 92  3 69 37 44 59 93 80 33 51 68 91 12 41 54
 67 25 18  9 81 10 48 73 15  5 97 11 78 76 22 32 82 57  6 64  7 36 86 71  0
  8 16 38 94 99 47 24 39 63 35 62 65 77 29 34  2 28 31 98 84 30 43  4 83 40] - 8056728
Solver: RVND, number of workers: 1
Setting feeder affinity
I am the master. There are 1 mpi processes. (hostname = gpu01)
Roots [<pyDF.nodes.Feeder object at 0x7f9a3be95ad0>]
Starting 0
Main loop
I am worker 0
GPU0
set random seed!
Initial: 8056728-[45 55 87 13 19 23 58 53 74 14 75 90 17 66 70 72 42 56 46 50 89 21  1 96 60
 52 79 20 27 85 49 88 61 26 95 92  3 69 37 44 59 93 80 33 51 68 91 12 41 54
 67 25 18  9 81 10 48 73 15  5 97 11 78 76 22 32 82 57  6 64  7 36 86 71  0
  8 16 38 94 99 47 24 39 63 35 62 65 77 29 34  2 28 31 98 84 30 43  4 83 40]
Final time: 2.77348279953s - Best: 1078530-[45 68 14 62 15 95 85 84 17  5 39 82  2 58 63 42 76 53 90 38 72 83 40 78 12
 87 23 26  0 49 33 80 51  7 65 37 10 89 16 93 31 96 97 48 69 88 41 22 73 60
  6 71 56 30 77 98 44 32 67 13 81 24 19  8 54 25 86 91 47  1 99 29 34 50 18
 36 61 59 92 70  3 43 52  4  9 94 55 20 46 27 11 64 79 75 74 66 35 28 57 21]
Value - initial: 8056728, final: 1078530, improveup: 7.47010097077

data-line;i;8056728;f;1078530;t;2.77348279953;c;-2;fv;[1078530L];cv;-1;imp;7.47010097077;type;rvnd;inum;1;w;1

time;2.77348279953;man_time;0;neigh_time;0
man_time;0;man_merge_sol;0;man_best_sol;0;man_combine_sol;0

Waiting [0]
Terminating workers True 0 0
MPI TERMINATING
